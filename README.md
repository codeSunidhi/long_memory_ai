ğŸ§  Fast Long-Term Memory AI (Streamlit + Ollama + FAISS)

A lightweight AI chat application with long-term memory, built using:

ğŸ§  Ollama (phi model) for LLM responses

âš¡ FAISS for fast vector similarity search

ğŸ§® Simple local hash-based embeddings (no external embedding API)

ğŸ¨ Streamlit for the UI

This project demonstrates how to build a memory-augmented AI assistant running fully local.

ğŸš€ Features

Chat interface powered by Streamlit

Long-term memory storage

Vector similarity search using FAISS

Lightweight local embeddings (no OpenAI API required)

Automatic memory extraction from conversation

Displays retrieved memories for transparency

ğŸ“ Project Structure
LONG_MEMORY/
â”‚
â”œâ”€â”€ venv/                # Virtual environment
â”œâ”€â”€ app.py               # Main Streamlit application
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ pyvenv.cfg

âš™ï¸ Requirements

Python 3.8+

Ollama installed locally

phi model pulled in Ollama

ğŸ§© Install Ollama

Download and install Ollama:

ğŸ‘‰ https://ollama.com

Then pull the phi model:

ollama pull phi


Start Ollama server (if not already running):

ollama run phi


Or ensure the API is running at:

http://localhost:11434

ğŸ Setup Instructions
1ï¸âƒ£ Create Virtual Environment (Optional)
python -m venv venv


Activate it:

Windows

venv\Scripts\activate


Mac/Linux

source venv/bin/activate

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


If you don't have a requirements file yet, use:

streamlit
faiss-cpu
numpy
requests

3ï¸âƒ£ Run the App
streamlit run app.py


Open in browser:

http://localhost:8501

ğŸ§  How Memory Works
1. Memory Creation

When a user sends a message:

The LLM determines whether it contains important long-term information

If yes, it returns a short summary labeled as:

MEMORY: <summary>


That summary gets embedded and stored in FAISS

2. Memory Retrieval

Before generating a reply:

The system searches similar past memories

Injects relevant memories into the prompt

The assistant responds with contextual awareness

ğŸ› ï¸ Technical Overview
Embeddings

Uses a simple deterministic hash-based embedding:

np.random.seed(abs(hash(text)) % (2**32))
np.random.rand(384)


This is:

Fast

Fully local

Lightweight

Not semantically strong (demo purpose)

Vector Search

FAISS IndexFlatL2

Top-K retrieval (default: 3)

Prompt Structure

The assistant is forced to respond in this format:

MEMORY: <summary or NONE>

REPLY:
<assistant reply>


This allows:

Clean parsing

Automatic memory storage

Clear separation of reasoning vs response

ğŸ§ª Example Use Case

User:

I am training for a marathon in October.


Stored Memory:

User is training for a marathon in October.


Later:

How should I structure my week?


The assistant will recall marathon training context automatically.

ğŸ”§ Customization

You can modify:

Variable	Purpose
OLLAMA_MODEL	Change to another Ollama model
EMBED_DIM	Change embedding vector size
TOP_K	Number of memories retrieved
âš ï¸ Limitations

Embeddings are random hash-based (not semantic)

Memory resets when Streamlit session restarts

No persistent storage (RAM only)

Requires local Ollama server

ğŸ’¡ Future Improvements

Replace simple embeddings with:

SentenceTransformers

Ollama embeddings API

Add persistent storage (SQLite / JSON)

Add memory deletion/editing

Add conversation export

Deploy on cloud

ğŸ“œ License

Open-source project â€” free to use and modify.