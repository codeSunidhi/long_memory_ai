ğŸ§  Fast Long-Term Memory AI
(Streamlit + Ollama + FAISS)

A lightweight AI chat application with long-term memory, running fully local.

ğŸ”¥ Built With

ğŸ§  Ollama (phi model) â€” LLM responses

âš¡ FAISS â€” Fast vector similarity search

ğŸ§® Local hash-based embeddings â€” No external embedding API

ğŸ¨ Streamlit â€” Interactive UI

This project demonstrates how to build a memory-augmented AI assistant running completely offline.

ğŸš€ Features

ğŸ’¬ Chat interface powered by Streamlit

ğŸ§  Long-term memory storage

ğŸ” Vector similarity search using FAISS

ğŸ§® Lightweight local embeddings (no OpenAI API)

ğŸ“ Automatic memory extraction from conversation

ğŸ‘ Displays retrieved memories for transparency

ğŸ“ Project Structure

LONG_MEMORY
â”‚
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ venv/               Virtual environment

âš™ï¸ Requirements

Python 3.8+

Ollama installed locally

phi model pulled in Ollama

ğŸ§© Install Ollama

Download and install Ollama:

ğŸ‘‰ https://ollama.com

Then pull the model:

ollama pull phi


Start Ollama:

ollama run phi


Ensure the API is running at:

http://localhost:11434

ğŸ Setup Instructions
1ï¸âƒ£ Create Virtual Environment (Optional)
python -m venv venv


Activate it:

Windows
venv\Scripts\activate

Mac/Linux
source venv/bin/activate

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


If you don't have a requirements file yet:

pip install streamlit faiss-cpu numpy requests

3ï¸âƒ£ Run the App
streamlit run app.py


Open in browser:

http://localhost:8501

ğŸ§  How Memory Works
ğŸ”¹ Memory Creation

When a user sends a message:

The LLM checks if it contains long-term information

If yes, it returns a short summary labeled:

MEMORY:


That summary is embedded and stored in FAISS

ğŸ”¹ Memory Retrieval

Before generating a reply:

The system searches similar past memories

Injects relevant memories into the prompt

The assistant responds with contextual awareness

ğŸ› ï¸ Technical Overview
ğŸ§® Embeddings

Uses deterministic hash-based embedding:

np.random.seed(abs(hash(text)) % (2**32))
np.random.rand(384)


This approach is:

âš¡ Fast

ğŸ  Fully local

ğŸª¶ Lightweight

âš ï¸ Not semantically strong (demo purpose)

ğŸ” Vector Search

FAISS IndexFlatL2

Top-K retrieval (default: 3)

ğŸ§© Prompt Structure

The assistant is forced to respond in this format:

MEMORY:
REPLY:


This enables:

Clean parsing

Automatic memory storage

Clear reasoning vs response separation

ğŸ§ª Example Use Case

User:

I am training for a marathon in October.

Stored Memory:

User is training for a marathon in October.

Later...

User:

How should I structure my week?

The assistant automatically recalls marathon training context.

ğŸ”§ Customization
Variable	Purpose
OLLAMA_MODEL	Change to another Ollama model
EMBED_DIM	Change embedding vector size
TOP_K	Number of memories retrieved
âš ï¸ Limitations

Embeddings are random hash-based (not semantic)

Memory resets when Streamlit session restarts

No persistent storage (RAM only)

Requires local Ollama server

ğŸ’¡ Future Improvements

Replace simple embeddings with:

SentenceTransformers

Ollama Embeddings API

Add persistent storage (SQLite / JSON)

Add memory deletion/editing

Add conversation export

Deploy on cloud

ğŸ“œ License

Open-source project â€” free to use and modify.
